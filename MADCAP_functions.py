#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import scipy.stats as st
from datetime import datetime as dt
from datetime import timedelta
import warnings
from netCDF4 import Dataset
from matplotlib import pyplot as plt
import hashlib
import fnmatch
import pickle
from glob import glob
import os

def loadVARSnetCDF(filePath, varNames=None, verbose=False):
    badDataCuttoff = 1e12 # values larger than this will be replaced with NaNs
    if varNames: assert isinstance(varNames, (list, np.ndarray)), 'varNames must be a list or numpy array!'
    measData = dict()
    netCDFobj = Dataset(filePath)
    if varNames is None: varNames = netCDFobj.variables.keys()
    for varName in varNames:
        if varName in netCDFobj.variables.keys():
            with warnings.catch_warnings(): 
                warnings.simplefilter('ignore', category=UserWarning) # ignore missing_value not cast warning
                measData[varName] = np.array(netCDFobj.variables[varName])
            if 'float' in measData[varName].dtype.name and np.any(measData[varName] > badDataCuttoff):
                if np.issubdtype(measData[varName].dtype, np.integer): # numpy ints can't be NaN
                    measData[varName] = measData[varName].astype(np.float32)
                measData[varName][measData[varName] > badDataCuttoff] = np.nan
        elif verbose:
            print("\x1b[1;35m Could not find \x1b[1;31m%s\x1b[1;35m variable in netCDF file: %s\x1b[0m" % (varName,filePath))
    netCDFobj.close()
    return measData

def hashFileSHA1(filePath):
    BLOCKSIZE = 65536
    hasher = hashlib.sha1()
    with open(filePath, 'rb') as afile:
        buf = afile.read(BLOCKSIZE)
        while len(buf) > 0:
            hasher.update(buf)
            buf = afile.read(BLOCKSIZE)
    return hasher.hexdigest()

def findNewestMatch(directory, pattern='*'):
    nwstTime = 0
    for file in os.listdir(directory):
        filePath = os.path.join(directory, file)
        if fnmatch.fnmatch(file, pattern) and os.path.getmtime(filePath) > nwstTime:
            nwstTime = os.path.getmtime(filePath)
            newestFN = filePath 
    if nwstTime > 0:
        return newestFN
    else:
        return ''
    
def ordinal2datetime(ordinal):
    dtObjDay = dt.fromordinal(np.int(np.floor(ordinal)))
    dtObjTime = timedelta(seconds=np.remainder(ordinal, 1)*86400)
    dtObj = dtObjDay + dtObjTime
    return dtObj

def KDEhist2D(x,y, axHnd=None, res=100, xrng=None, yrng=None, sclPow=1, cmap = 'BuGn', clbl='Probability Density (a.u.)'):
    # set plot range
    xmin = xrng[0] if xrng else x.min()
    xmax = xrng[1] if xrng else x.max()
    ymin = yrng[0] if yrng else y.min()
    ymax = yrng[1] if yrng else y.max()
    # Peform the kernel density estimate
    xx, yy = np.mgrid[xmin:xmax:complex(0,res), ymin:ymax:complex(0,res)]
    positions = np.vstack([xx.ravel(), yy.ravel()])
    values = np.vstack([x, y])
    kernel = st.gaussian_kde(values)
    f = np.reshape(kernel(positions).T, xx.shape)
    # Plot results
    if not axHnd:
        fig = plt.figure()
        axHnd = fig.gca()
    axHnd.set_xlim(xmin, xmax)
    axHnd.set_ylim(ymin, ymax)
    # Contourf plot
    objHnd = axHnd.contourf(xx, yy, f**sclPow, 256, cmap=cmap)
    clrHnd = plt.colorbar(objHnd, ax=axHnd)
#    objHnd.set_clim(vmin=0)
    tckVals = clrHnd.get_ticks()**(1/sclPow)/np.max(clrHnd.get_ticks()**(1/sclPow))
    clrHnd.set_ticklabels(['%4.1f' % x for x in 100*tckVals])
    clrHnd.set_label(clbl)
    return axHnd
    
def readPetesAngleFiles(filePtrn, nAng=10, verbose=False, saveData=True, loadData=True):
    """
    Read in the randomly sampled angle text files generated by Pete.
    filePtrn - Full path directory from which glob will grab ALL text files (should be free of extraneous *.txt files) 
    nAng - The number of viewing angles per ground pixel
    saveData - Save the results to the directory in filePtrn in the form of a pickle
    loadData - Load the results from a previous run, if they exist
    """
    pklPath = filePtrn.replace('*','ALL')[:-3]+'pkl'
    if loadData == True:
        try:
            with open(pklPath, 'rb') as f:
                angDict = pickle.load(f)
            if verbose: print('Data loaded from %s' % pklPath)
            return angDict
        except EnvironmentError:
            if verbose: print('Could not load valid pickle data from %s. Processing raw text files...' % pklPath)    
    fileNames = np.sort(glob(os.path.join(filePtrn, '*.txt')))
    angDict = {k: [] for k in ['lon','lat','datetime','vis','sza','fis','sca']}
    for fn in fileNames:
        if verbose: print('Processing %s...' % os.path.basename(fn))
        peteData = np.genfromtxt(fn, delimiter=',', skip_header=1, dtype=None, encoding='UTF8')
        for ind in range(0, len(peteData), nAng):
            angDict['lon'].append(peteData[ind][0])
            angDict['lat'].append(peteData[ind][1])
            dtStr = peteData[ind][2] + '%02d' % np.floor(95/60) + '%02d' % (95 % 60)
            angDict['datetime'].append(dt.strptime(dtStr, '%Y%m%d_%H00z%M%S'))
            angDict['vis'].append([x[5] for x in peteData[ind:ind+nAng]])
            angDict['sza'].append(np.mean([x[6] for x in peteData[ind:ind+nAng]]))
            fis = np.array([x[8]-x[7] for x in peteData[ind:ind+nAng]])
            fis[fis<0] = fis[fis<0] + 360
            angDict['fis'].append(fis)
            angDict['sca'].append([x[9] for x in peteData[ind:ind+nAng]])
    for key in angDict.keys(): angDict[key] = np.array(angDict[key]) # convert everything to numpy
    if saveData:
        with open(pklPath, 'wb') as f:
            pickle.dump(angDict, f, pickle.HIGHEST_PROTOCOL)
        if verbose: print('Data saved to %s' % pklPath)
    return angDict